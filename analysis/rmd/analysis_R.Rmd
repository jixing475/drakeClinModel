---
title: "Untitled"
author: "Jixing Liu"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output:
  html_notebook:
    theme: united
    highlight: zenburn
    toc: true
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,      # Output code chunks
    message = TRUE,  # Toggle off message output 
    warning = TRUE,    # Toggle off warning output
    fig.width = 6, fig.asp = 0.618, out.width = "70%", fig.align = "center") 

knitr::opts_knit$set(root.dir = usethis::proj_path())
#library(docknitr)

# libraries used in report
library(knitr)
library(kableExtra)
library(tidyverse)

# Globql formatting options
options(digits = 3)

# Global table settings 
options(DT.options = list(pageLength = 10, 
                          language = list(search = 'Filter:'), 
                          scrollX = TRUE))
# Global ggplot settings
theme_set(theme_light(base_family = "Avenir"))
```


## conda env
```{bash eval=FALSE, include=TRUE}
conda create -n clinModel -y python==3.7
conda activate clinModel

conda install pandas matplotlib numpy scikit-learn xgboost  seaborn
conda install xlrd

#pip3 install graphviz
conda install graphviz
```



```{r set python}
library(reticulate)
reticulate::use_python("/Users/zero/anaconda3/envs/clinModel/bin/python", required = TRUE)
reticulate::py_config()
# knitr::knit_engines$set(python = reticulate::eng_python) è¿™ä¸ªè®¾ç½®åè€Œä¼šé€ æˆ chunk ä¹‹é—´çš„ R å’Œ python çš„å˜é‡æ²¡èƒ½äº’é€š
```

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gj4x2lg6lgj31cq0ioadz.jpg)

## set up

### utils setup


```{python}
# -- coding:utf-8 --
import os
from os.path import join as pjoin
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report

```


### utils_features_selection: setup
```{python}
# -- coding:utf-8 --
import pandas as pd
import numpy as np
import os
from os.path import join as pjoin

# from utils import is_number

import matplotlib.pyplot as plt
import seaborn as sns
import warnings

import xgboost as xgb
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from matplotlib import pyplot as plt
from matplotlib.colors import ListedColormap

from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn import preprocessing

from mpl_toolkits.mplot3d import Axes3D
# import utils
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve, auc
import matplotlib as mpl

warnings.filterwarnings('ignore')
#%matplotlib inline
sns.set_style("white")

plt.rcParams['font.sans-serif']=['Simhei']
plt.rcParams['axes.unicode_minus']=False

```

### Main_of_features_selection: setup

```{python}
# -- coding:utf-8 --
import pandas as pd
import numpy as np

#from utils_features_selection import *
# è‡ªå®šä¹‰çš„æ‰€æœ‰å‡½æ•°

from xgboost import plot_tree

from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn import tree

```



## function

### is_number

```{python}
def is_number(s):
    if s is None:
        s = np.nan

    try:
        float(s)
        return True
    except ValueError:
        pass

    try:
        import unicodedata
        unicodedata.numeric(s)
        return True
    except (TypeError, ValueError):
        pass

    return False
```

### read_train_data
```{python}
################################
## Read data functions
###############################
def read_train_data(path_train):
    data_df = pd.read_excel(path_train, index_col=[0, 1])  # train_sample_375_v2 train_sample_351_v4
    data_df = data_df.groupby('PATIENT_ID').last()
    # data_df = data_df.iloc[:,1:]
    # data_df = data_df.set_index(['PATIENT_ID'])
    # data_df['å¹´é¾„'] = data_df['å¹´é¾„'].apply(lambda x: x.replace('å²', '') if is_number(x.replace('å²', '')) else np.nan).astype(float)
    # data_df['æ€§åˆ«'] = data_df['æ€§åˆ«'].map({'ç”·': 1, 'å¥³': 2})
    # data_df['æŠ¤ç†->å‡ºé™¢æ–¹å¼'] = data_df['æŠ¤ç†->å‡ºé™¢æ–¹å¼'].map({'æ²»æ„ˆ': 0,'å¥½è½¬': 0, 'æ­»äº¡': 1})
    lable = data_df['å‡ºé™¢æ–¹å¼'].values
    data_df = data_df.drop(['å‡ºé™¢æ–¹å¼', 'å…¥é™¢æ—¶é—´', 'å‡ºé™¢æ—¶é—´'], axis=1)
    data_df['Type2'] = lable
    data_df = data_df.applymap(lambda x: x.replace('>', '').replace('<', '') if isinstance(x, str) else x)
    data_df = data_df.applymap(lambda x: x if is_number(x) else -1)
    # data_df = data_df.loc[:, data_df.isnull().mean() < 0.2]
    data_df = data_df.astype(float)

    return data_df

# df = read_train_data(path_train)
```

### merge_data_by_sliding_window
```{python}
def merge_data_by_sliding_window(data, n_days=1, dropna=True, subset=None, time_form='diff'):
    """æ»‘çª—åˆå¹¶æ•°æ®

    :param data: æ—¶é—´åºåˆ—æ•°æ®ï¼Œä¸€çº§è¡Œç´¢å¼•ä¸º PATIENT_ID, äºŒçº§è¡Œç´¢å¼•ä¸º RE_DATE
    :param n_days: çª—å£é•¿åº¦
    :param dropna: æ»‘çª—åˆå¹¶åè¿˜ç¼ºå¤±çš„æ˜¯å¦åˆ æ‰
    :param subset: pd.DataFrame().dropna() å‚æ•°                                                   Note: æ–°å‚æ•°!
    :param time_form: è¿”å›æ•°æ®çš„æ—¶é—´ç´¢å¼•ï¼Œ'diff' or 'timestamp'
    :return: åˆå¹¶åçš„æ•°æ®ï¼Œä¸€çº§è¡Œç´¢å¼•ä¸º PATIENT_ID, äºŒçº§è¡Œç´¢å¼•ä¸º t_diff or RE_DATE, å–å†³äº"time_form"
    """
    #æ ¹æ®PATIENT_IDæ’åº
    data = data.reset_index(level=1)
    # dt.normalize() å–å‡ºé™¢æ—¶é—´çš„å¤©æ•°
    # è·ç¦»å‡ºé™¢æ—¶é•¿        Note: å»æ‰äº†å‡ºé™¢æ—¶é—´å’Œæ£€æµ‹æ—¶é—´çš„æ—¶åˆ†ç§’ï¼Œå› ä¸ºæˆ‘è§‰å¾—ä»¥ 00:00:00 ä¸ºåˆ†ç•Œç‚¹æ›´åˆé€‚
    t_diff = data['å‡ºé™¢æ—¶é—´'].dt.normalize() - data['RE_DATE'].dt.normalize()
    # æ»‘çª—å–æ•´çš„ä¾æ®ã€‚å³nn_dayså¤©å†…çš„ä¼šå–æ•´æˆä¸ºåŒä¸€ä¸ªæ•°å€¼ï¼Œåé¢é€šè¿‡groupbyæ–¹æ³•åˆ†ç»„
    data['t_diff'] = t_diff.dt.days.values // n_days * n_days
    #
    data = data.set_index('t_diff', append=True)

    # æ»‘çª—åˆå¹¶ã€‚å¯¹['PATIENT_ID', 't_diff']groupbyï¼Œç›¸å½“äºåŒå¾ªç¯ã€‚éå†æ‰€æœ‰ç—…äººä¸ç—…äººçš„æ‰€æœ‰çª—å£
    # å› ä¸ºä¹‹å‰å¯¹dataæ’åºï¼Œå› æ­¤æ¯ä¸ªç—…äººt_diffä¼šæ˜¯ä»å¤§åˆ°å°çš„æ’åº,ffill()æ˜¯å‘ä¸Šä¸€è¡Œæ’å€¼ï¼Œå› æ­¤ç›¸å½“äºæ˜¯å‘æ—§æ—¥æœŸæ’å€¼
    # last()æ˜¯æ¯ä¸€ç»„å–æœ€åä¸€è¡Œï¼Œå› æ­¤å³å–æ¯ä¸ªç—…äººå¯¹åº”çª—å£çš„æœ€åä¸€æ¬¡æ•°æ®ï¼Œï¼ˆä¹Ÿä¸€å®šæ˜¯æœ€å…¨çš„ï¼‰ã€‚
    # last(ï¼‰è‡ªå¸¦æ’åºã€‚å–å®Œlaståä¼šæŒ‰ç…§ç´¢å¼•å‡åºæ’åˆ—
    data = (
        data
        .groupby(['PATIENT_ID', 't_diff']).ffill()
        .groupby(['PATIENT_ID', 't_diff']).last()
    )
    # å»æ‰ç¼ºå¤±æ ·æœ¬
    if dropna:
        data = data.dropna(subset=subset)         # Note: è¿™é‡Œå¯¹ç¼ºå¤±å€¼è¿›è¡Œäº† dropna(), è€Œä¸æ˜¯ fillna(-1)

    # æ›´æ–°äºŒçº§ç´¢å¼•ã€‚ï¼ˆå…¶å®timestampåœ¨æœ¬è®ºæ–‡çš„ä¸­æ²¡ç”¨åˆ°ï¼‰
    if time_form == 'timestamp':
        data = (
            data
            .reset_index(level=1, drop=True)
            .set_index('RE_DATE', append=True)
        )
    elif time_form == 'diff':
        data = data.drop(columns=['RE_DATE'])

    return data
```

### data_preprocess

`data_df_unna`: è·å–375ç—…äººï¼ˆdata_df_unnaï¼‰ 
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gj6jhvuk7kj31i40ki0ys.jpg)


`data_pre_df`: å’Œ110ç—…äººï¼ˆdata_pre_dfï¼‰
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1gj6jibz716j31i40kigqs.jpg)





```{python}
def data_preprocess(path_train, path_test):
    #path_train = './data/time_series_375_prerpocess.xlsx'  # to_ml
    data_df_unna = read_train_data(path_train)

    # data_pre_df = pd.read_csv('./data/sample29_v3.csv',encoding='gbk')
    data_pre_df = pd.read_excel(path_test, index_col=[0, 1])
    data_pre_df = merge_data_by_sliding_window(data_pre_df, n_days=1, dropna=True, subset=top3_feats_cols,
                                                     time_form='diff')
    data_pre_df = data_pre_df.groupby('PATIENT_ID').first().reset_index()
    data_pre_df = data_pre_df.applymap(lambda x: x.replace('>', '').replace('<', '') if isinstance(x, str) else x)
    data_pre_df = data_pre_df.drop_duplicates()

    return data_df_unna, data_pre_df
```

### calculate miss values by col
```{python}
## calculate miss values by col
def col_miss(train_df):
    col_missing_df = train_df.isnull().sum(axis=0).reset_index()
    col_missing_df.columns = ['col','missing_count']
    col_missing_df = col_missing_df.sort_values(by='missing_count')
    return col_missing_df
```

### Data read and split

```{python}
def data_read_and_split(is_dropna=False,sub_cols=None):
    # data_df_unnaä¸º375æ•°æ®é›†ï¼Œdata_pre_dfä¸º110æ•°æ®é›†
    data_df_unna,data_pre_df = data_preprocess(path_train, path_test)
    if is_dropna==True:
        data_df_unna = data_df_unna.dropna(subset=sub_cols,how='any')

    # è®¡ç®—ç‰¹å¾çš„ç¼ºå¤±æƒ…å†µ
    col_miss_data = col_miss(data_df_unna)
    # è®¡ç®—ç‰¹å¾ç¼ºå¤±æ¯”ä¾‹
    col_miss_data['Missing_part'] = col_miss_data['missing_count']/len(data_df_unna)
    # é€‰æ‹©ç¼ºå¤±å°‘äº0.2çš„ç‰¹å¾
    sel_cols = col_miss_data[col_miss_data['Missing_part']<=0.2]['col']
    # copyå‡½æ•°å°†é€‰æ‹©çš„ç‰¹å¾æ•°æ®æ‘˜å‡ºæ¥ï¼Œä¸å½±å“åŸæ•°æ®çš„æ•°å€¼
    data_df_sel = data_df_unna[sel_cols].copy()
    # è®¡ç®—æ‰€æœ‰ç‰¹å¾
    cols = list(data_df_sel.columns)
    # å‰”é™¤å¹´é¾„å’Œæ€§åˆ«
    cols.remove('å¹´é¾„')
    cols.remove('æ€§åˆ«')
    cols.remove('Type2')
    cols.append('Type2')
    # æ„é€ å‰”é™¤ä¸Šè¿°ç‰¹å¾çš„dataframe
    data_df_sel2 = data_df_sel[cols]
    # æ–°å»ºä¸€ä¸ªdataframe
    data_df_unna = pd.DataFrame()
    # ç±»ä¼¼copyæ–¹æ³•ï¼Œæ–°å»ºå˜é‡ï¼Œä¿®æ”¹ä¸ä¼šå½±å“åŸæ•°å€¼
    data_df_unna = data_df_sel2

    # å¯¹ç¼ºå¤±æ•°å€¼æ·»-1
    data_df_unna = data_df_unna.fillna(-1)

    # å–å‡ºç‰¹å¾åï¼Œä»ç¬¬ä¸€åˆ—åˆ°å€’æ•°ç¬¬äºŒåˆ—
    x_col = cols[:-1]
    #print(x_col)
    # å–å‡ºæ ‡ç­¾å
    y_col = cols[-1]
    #å–å‡º375ç‰¹å¾æ•°æ®
    X_data = data_df_unna[x_col]#.values
    #å–å‡º375æ ‡ç­¾æ•°æ®
    Y_data = data_df_unna[y_col]#.values

    return X_data,Y_data,x_col

```
### StratifiedKFold_func
```{python}

def StratifiedKFold_func(x, y,Num_iter=100,model = xgb.XGBClassifier(max_depth=4,learning_rate=0.2,reg_alpha=1), score_type ='auc'):
    # æ¨¡å‹åœ¨å¾ªç¯å¤–çš„kæŠ˜
    # åˆ†å±‚ K æŠ˜äº¤å‰éªŒè¯
    acc_v = []
    acc_t = []
    
    for i in range(Num_iter):
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)
        for tr_idx, te_idx in skf.split(x,y):
            x_tr = x[tr_idx, :]
            y_tr = y[tr_idx]
            x_te = x[te_idx, :]
            y_te = y[te_idx]

            model.fit(x_tr, y_tr)
            pred = model.predict(x_te)
            train_pred = model.predict(x_tr)

            pred_Proba = model.predict_proba(x_te)[:,1]
            train_pred_Proba = model.predict_proba(x_tr)[:,1]

            if score_type == 'auc':
            	acc_v.append(roc_auc_score(y_te, pred_Proba))
            	acc_t.append(roc_auc_score(y_tr, train_pred_Proba))
            else:
            	acc_v.append(f1_score(y_te, pred))
            	acc_t.append(f1_score(y_tr, train_pred))            	

    return [np.mean(acc_t), np.mean(acc_v), np.std(acc_t), np.std(acc_v)]
```


### StratifiedKFold_func_with_features_sel

è¿™ä¸ªå‡½æ•°çš„ä½œç”¨ä¸»è¦æ˜¯ç”¨ KæŠ˜çš„åŠæ³•ç®—ä¸€äº›æŒ‡æ ‡  

è¾“å…¥: X, y(ç‰¹å¾å’Œç›®æ ‡)  

è¾“å‡º: æŒ‡æ ‡çš„å¹³å‡å€¼å’Œæ ‡å‡†å·®

```{python}
def StratifiedKFold_func_with_features_sel(x, y,Num_iter=100,score_type = 'auc'):
    # åˆ†å±‚ K æŠ˜äº¤å‰éªŒè¯
    acc_v = []
    acc_t = []
    # æ¯æ¬¡KæŠ˜100æ¬¡ï¼
    for i in range(Num_iter):
        # æ¯æ¬¡æŠ˜æ˜¯éšæœºçš„random_state=i
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=i)
        for tr_idx, te_idx in skf.split(x,y):
            x_tr = x[tr_idx, :]
            y_tr = y[tr_idx]
            x_te = x[te_idx, :]
            y_te = y[te_idx]
            #å®šä¹‰æ¨¡å‹è¶…å‚æ•°
            model = xgb.XGBClassifier(max_depth=4,learning_rate=0.2,reg_alpha=1)
            #æ¨¡å‹æ‹Ÿåˆ
            model.fit(x_tr, y_tr)
            pred = model.predict(x_te)
            train_pred = model.predict(x_tr)
            #è°ƒç”¨sklearn çš„roc_auc_score ä¸f1_scoreè®¡ç®—ç›¸å…³æŒ‡æ ‡
            # 1. accuracy_score
            # 2. recall_score
            # 3. f1_score
            # 4. roc_auc_score
            ## æ³¨æ˜Læ­¤å¤„ç”¨é¢„æµ‹çš„æ ‡ç­¾å€¼è€Œä¸æ˜¯é¢„æµ‹æ¦‚ç‡æ±‚çš„AUC,åŸå› æ˜¯å› ä¸ºæœ¬æ–‡ç€é‡è€ƒè™‘é¢„æµ‹åŒºåˆ†ç”Ÿæ­»ï¼Œè¿ç”¨é¢„æµ‹æ ‡ç­¾ç›¸å½“äºåœ¨é˜ˆå€¼ç¡®å®šä¸º0.5çš„æƒ…å†µä¸‹æ¨¡å‹çš„ç»“æœéªŒè¯ï¼Œ
            ## å…¶AUCé˜ˆå€¼åˆ†å‰²ç‚¹å¯è§†ä¸ºåˆ†åˆ«åœ¨1ï¼Œ0.5ï¼Œ0, è¿™æ ·æ›´èƒ½ååº”ç‰¹å¾çš„åŒºåˆ†æ€§èƒ½çš„å·®å¼‚æ€§ï¼Œæ‰¾å‡ºèƒ½æœ‰åŒºåˆ†åº¦è´¡çŒ®çš„ç‰¹å¾ã€‚
            if score_type == 'auc':
                acc_v.append(roc_auc_score(y_te, pred))
                acc_t.append(roc_auc_score(y_tr, train_pred))
            else:
                acc_v.append(f1_score(y_te, pred))
                acc_t.append(f1_score(y_tr, train_pred))    
    # è¿”å›å¹³å‡å€¼
    return [np.mean(acc_t), np.mean(acc_v), np.std(acc_t), np.std(acc_v)]
```

### show_confusion_matrix
```{python}
## Plot functions
######################
def show_confusion_matrix(validations, predictions):
    LABELS = ['Survival','Death']
    matrix = metrics.confusion_matrix(validations, predictions)
    # plt.figure(dpi=400,figsize=(4.5, 3))
    plt.figure(figsize=(4.5, 3))
    sns.heatmap(matrix,
                cmap='coolwarm',
                linecolor='white',
                linewidths=1,
                xticklabels=LABELS,
                yticklabels=LABELS,
                annot=True,
                fmt='d')
    plt.title('Confusion Matrix')
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()

```


### utils read

> è¯»å–å¤„ç†åçš„æ•°æ®, å¯ä»¥åˆå¹¶å¤„ç† parquet, csv, excel ä¸‰ç§æ ¼å¼æ–‡ä»¶çš„è¯»å–å‡½æ•°

```{python}
def read(path: str, usecols=None, is_ts='infer'):
    """è¯»å–å¤„ç†åçš„æ•°æ®
    åˆå¹¶ parquet, csv, excel ä¸‰ç§æ ¼å¼æ–‡ä»¶çš„è¯»å–å‡½æ•°

    :param path: æ–‡ä»¶è·¯å¾„ï¼Œå¿…é¡»æ˜¯ parquet æˆ– csv æˆ– excel æ–‡ä»¶
    :param usecols: é€‰å–çš„åˆ—ã€‚ä¸ pandas æ¥å£ä¸åŒï¼Œæ­¤å¤„è¿›è¡Œäº†ç®€åŒ–ï¼Œæ— éœ€å†™ç´¢å¼•åˆ—
    :param is_ts: æ˜¯å¦ä¸ºæ—¶é—´åºåˆ—ã€‚å¯é€‰å€¼ï¼š'infer', True, False
    :return: è¯»å–çš„ DateFrame æ•°æ®
    """
    # è®¾ç½®ç´¢å¼•
    if is_ts == 'infer':
        index_col = [0, 1] if os.path.split(path)[1].startswith('time_series') else [0]
    elif is_ts is True:
        index_col = [0, 1]
    elif is_ts is False:
        index_col = [0]
    else:
        raise Exception('is_ts å‚æ•°é”™è¯¯')

    # è¯»å–æ•°æ®
    if path.endswith('.parquet'):
        data = pd.read_parquet(path)
    elif path.endswith('.csv'):
        try:
            data = pd.read_csv(path, index_col=index_col, encoding='gb18030')
        except UnicodeDecodeError:
            data = pd.read_csv(path, index_col=index_col, encoding='utf-8')
        except:
            raise
    elif path.endswith('.xlsx'):
        data = pd.read_excel(path, index_col=index_col)
    else:
        raise Exception('æ–‡ä»¶ç±»å‹é”™è¯¯')

    # æå–æŒ‡å®šåˆ—
    if usecols is not None:
        data = data[usecols]

    return data
```


### utils score_form

```{python}

def score_form(x: np.array):
    """æ‰“åˆ†è¡¨é¢„æµ‹
    example: pred, score = score_form(df[['ä¹³é…¸è„±æ°¢é…¶', 'æ·‹å·´ç»†èƒ(%)', 'è¶…æ•Cååº”è›‹ç™½']].values)

    :param x: åˆ—é¡ºåºï¼š['ä¹³é…¸è„±æ°¢é…¶', 'æ·‹å·´ç»†èƒ(%)', 'è¶…æ•Cååº”è›‹ç™½']
    :return: é¢„æµ‹ç±»åˆ«åŠæœ€åå¾—åˆ†
    """
    x = x.copy()

    # ä¹³é…¸è„±æ°¢é…¶
    x[:, 0] = pd.cut(
        x[:, 0],
        [-2, 107, 159, 210, 262, 313, 365, 416, 467, 519, 570, 622, 673, 724, 776, 827, 1e5],
        labels=list(range(-5, 11))
    )

    # æ·‹å·´ç»†èƒ(%)
    x[:, 1] = pd.cut(
        x[:, 1],
        [-2, 1.19, 3.12, 5.05, 6.98, 8.91, 10.84, 12.77, 14.7, 16.62, 18.55, 20.48, 22.41, 24.34, 1e5],
        labels=list(range(8, -6, -1))
    )

    # è¶…æ•Cååº”è›‹ç™½
    x[:, 2] = pd.cut(
        x[:, 2],
        [-2, 19.85, 41.2, 62.54, 83.88, 1e5],
        labels=list(range(-1, 4))
    )

    # ç»Ÿåˆ†
    total_score = x.sum(axis=1)

    # 1 åˆ†ä¸ºä¸´ç•Œç‚¹ï¼Œå¤§äº 1 åˆ†æ­»äº¡ï¼Œå°äº 1 åˆ†æ²»æ„ˆ
    pred = (total_score > 1).astype(int)
    return pred, total_score
```

### utils decision_tree

```{python}
def decision_tree(x: pd.Series):
    """æ­£æ–‡ä¸­çš„å†³ç­–æ ‘
    example: df.apply(decision_tree, axis=1)

    :param x: å•ä¸ªæ ·æœ¬ï¼Œ['ä¹³é…¸è„±æ°¢é…¶', 'è¶…æ•Cååº”è›‹ç™½', 'æ·‹å·´ç»†èƒ(%)']
    :return: 0: æ²»æ„ˆ, 1: æ­»äº¡
    """
    if x['ä¹³é…¸è„±æ°¢é…¶'] >= 365:
        return 1

    if x['è¶…æ•Cååº”è›‹ç™½'] < 41.2:
        return 0

    if x['æ·‹å·´ç»†èƒ(%)'] > 14.7:
        return 0
    else:
        return 1
```


### utilsget_time_in_advance_of_predict

```{python}
def get_time_in_advance_of_predict(data):
    """æå‰é¢„æµ‹æ­£ç¡®çš„å¤©æ•°

    :param data: æ—¶é—´åºåˆ—æ•°æ®ï¼Œä¸€çº§è¡Œç´¢å¼•ä¸º PATIENT_ID, äºŒçº§è¡Œç´¢å¼•ä¸º t_diff
    :return: pd.Series, index: PATIENT_ID, values: æå‰é¢„æµ‹æ­£ç¡®çš„å¤©æ•°
    """
    # ç”±äºpythonçš„æœºåˆ¶ï¼Œç”¨copyæ–°å»ºä¸€ä¸ªdataï¼Œä¸ç„¶ä¼šä¿®æ”¹åŸdat
    data = data.copy()
    # åœ¨data è¿™ä¸ªdataframeä¸­æ–°å»ºä¸€åˆ—rightï¼Œæ•°å€¼æ˜¯åˆ¤å®šæ˜¯å¦æ­£ç¡®
    data['right'] = data['pred'] == data['å‡ºé™¢æ–¹å¼']
    # æ–°å»ºä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œç”¨äºå­˜å‚¨æå‰é¢„æµ‹çš„æ­£ç¡®çš„å¤©æ•°
    time_advance = []
    # data.index.remove_unused_levels().levels[0]è¡¨ç¤ºçš„æ˜¯ç—…äººidçš„listï¼Œå³éå†æ‰€æœ‰ç—…äºº
    for id_ in data.index.remove_unused_levels().levels[0]:
        # å› ä¸ºç—…äººidæ˜¯ä¸€çº§ç´¢å¼•ï¼Œlocæ–¹æ³•å–å‡ºè¯¥ç—…äººå¯¹åº”çš„æ‰€æœ‰æ•°æ®ï¼ˆå¯èƒ½æœ‰å¤šæ¡ï¼‰
        d = data.loc[id_]
        # å¦‚æœç—…äººåªæœ‰ä¸€æ¡æ•°æ®å•æ•°æ®
        if len(d) == 1:
            if d.iloc[0]['right']:
                # å°†é¢„æµ‹å¯¹å­˜å…¥time_advanceï¼Œåˆ†åˆ«ä¸ºç—…äººçš„idï¼Œæ­£ç¡®çš„å¤©æ•°ï¼Œå‡ºé™¢çš„æ–¹å¼
                time_advance.append([id_, d.iloc[0].name, d['å‡ºé™¢æ–¹å¼'].iat[0]])
            continue

        # å¤šæ•°æ® Step1: é¢„æµ‹é”™
        if not d.iloc[0]['right']:
            continue

        # å¤šæ•°æ® Step2: å…¨å¯¹
        if d['right'].all():
            # å°†é¢„æµ‹å¯¹å­˜å…¥time_advanceï¼Œåˆ†åˆ«ä¸ºç—…äººçš„idï¼Œæ­£ç¡®çš„å¤©æ•°ï¼Œå‡ºé™¢çš„æ–¹å¼
            time_advance.append([id_, d.iloc[-1].name, d['å‡ºé™¢æ–¹å¼'].iat[0]])
            continue

        # å¤šæ•°æ® Step3: éƒ¨åˆ†å¯¹
        for i in range(len(d)):
            if d.iloc[i]['right']:
                continue
            else:
                # å°†é¢„æµ‹å¯¹å­˜å…¥time_advanceï¼Œåˆ†åˆ«ä¸ºç—…äººçš„idï¼Œæ­£ç¡®çš„å¤©æ•°ï¼Œå‡ºé™¢çš„æ–¹å¼
                time_advance.append([id_, d.iloc[i-1].name, d['å‡ºé™¢æ–¹å¼'].iat[0]])
                break

    # å°†time_advanceå­˜æˆDataFrame
    time_advance = pd.DataFrame(time_advance, columns=['PATIENT_ID', 'time_advance', 'outcome'])
    time_advance = time_advance.set_index('PATIENT_ID')
    return time_advance


class Metrics:
    def __init__(self, report=None, acc=None, f1=None, conf_mat=None):
        self.y_trues  = []
        self.y_preds  = []

        # list or None. 'every': æ¯æŠ˜éƒ½æ‰“å°; 'overall': æ‰“å°æ€»ä½“çš„
        if isinstance(report, list):
            self.report = report
        else:
            self.report = [report]

        if isinstance(acc, list):
            self.acc = acc
        else:
            self.acc = [acc]

        if isinstance(f1, list):
            self.f1 = f1
        else:
            self.f1 = [f1]

        if isinstance(conf_mat, list):
            self.conf_mat = conf_mat
        else:
            self.conf_mat = [conf_mat]

    def record(self, y_true, y_pred):
        self.y_trues.append(y_true)
        self.y_preds.append(y_pred)
        return self

    def clear(self):
        self.y_trues = []
        self.y_preds = []
        return self

    def print_metrics(self):
        """æ‰“å°æŒ‡æ ‡

        :param report:
        :param acc:
        :param f1:
        :param conf_mat:
        :return:
        """
        # Loop: 'every'
        acc_values, f1_values = [], []
        single_fold = True if len(self.y_trues) == 1 else False
        for i, (y_true, y_pred) in enumerate(zip(self.y_trues, self.y_preds)):
            assert (y_true.ndim == 1) and (y_pred.ndim == 1)
            print(f'\n======================== ç¬¬ {i+1} æŠ˜æŒ‡æ ‡ ========================>')

            # Classification_report
            if (self.report is not None) and ('every' in self.report):
                print(classification_report(y_true, y_pred))

            # Accuracy_score
            a_v = accuracy_score(y_true, y_pred)
            acc_values.append(a_v)
            if (self.acc is not None) and ('every' in self.acc):
                print(f"accuracy: {a_v:.05f}")

            # F1_score
            f1_v = f1_score(y_true, y_pred, average='macro')
            f1_values.append(f1_v)
            if (self.f1 is not None) and ('every' in self.f1):
                print(f"F1: {f1_v:.05f}")

            # Confusion_matrix
            if (self.conf_mat is not None) and ('every' in self.conf_mat):
                print(f"æ··æ·†çŸ©é˜µï¼š\n{confusion_matrix(y_true, y_pred)}")

        # 'Overall'
        print('\n======================== æ€»ä½“æŒ‡æ ‡ ========================>')
        y_true = np.hstack(self.y_trues)
        y_pred = np.hstack(self.y_preds)

        # Classification_report
        if (self.report is not None) and ('overall' in self.report):
            print(classification_report(y_true, y_pred))

        # Accuracy_score
        if (self.acc is not None) and ('overall' in self.acc):
            if single_fold:
                print(f"accuracyï¼š\t{acc_values[0]: .04f}")
            else:
                print(f"accuracyï¼š\t{np.mean(acc_values): .04f} / {'  '.join([str(a_v.round(2)) for a_v in acc_values])}")

        # F1_score
        if (self.f1 is not None) and ('overall' in self.f1):
            if single_fold:
                print(f"F1-scoreï¼š\t{f1_values[0]: .04f}")
            else:
                print(f"F1 å‡å€¼ï¼š\t{np.mean(f1_values): .04f} / {'  '.join([str(f1_v.round(2)) for f1_v in f1_values])}")

        # Confusion_matrix
        if (self.conf_mat is not None) and ('overall' in self.conf_mat):
            print(f"æ··æ·†çŸ©é˜µï¼š\n{confusion_matrix(y_true, y_pred)}")


```

### utils feat_zh2en

```{python}
def feat_zh2en(data):
    """ç‰¹å¾åä¸­æ–‡è½¬è‹±æ–‡"""
    feats_zh = data.columns

    # æ˜¾ç¤ºå“ªäº›åˆ—æ²¡æœ‰ä¸­è‹±ç¿»è¯‘
    feats_map = pd.read_excel('data/raw_data/ç‰¹å¾å_zh2en/ç‰¹å¾å_zh2en.xlsx', index_col=0)['en']
    out_of_map = set(feats_zh) - set(feats_map.index)
    print(f"ç¼ºå°‘ç¿»è¯‘çš„ç‰¹å¾ï¼š{out_of_map}")

    # å¼€å§‹ç¿»è¯‘
    feats_map = feats_map.to_dict()
    data = data.rename(columns=feats_map)
    return data

```

### util: plot_roc

```{python}
def plot_roc(labels, predict_prob,Moodel_name_i,fig,labels_name,k):
    false_positive_rate,true_positive_rate,thresholds=roc_curve(labels, predict_prob)
    roc_auc=auc(false_positive_rate, true_positive_rate)
    #plt.figure()
    line_list = ['--','-']
    ax = fig.add_subplot(111)
    plt.title('ROC', fontsize=20)
    ax.plot(false_positive_rate, true_positive_rate,line_list[k%2],linewidth=1+(1-k/5),label=Moodel_name_i+' AUC = %0.4f'% roc_auc)
    plt.xticks(fontsize=20)
    plt.yticks(fontsize=20)
    plt.ylabel('TPR', fontsize=20)
    plt.xlabel('FPR', fontsize=20)
    labels_name.append(Moodel_name_i+' AUC = %0.4f'% roc_auc)
    #plt.show()
    return labels_name

```
### utils concat_data

```{python}
def concat_data(data375: pd.DataFrame, data110: pd.DataFrame):
    """æ•´åˆ 375 + 110
    å› ä¸º PATIENT_ID éƒ½ä» 1 å¼€å§‹ï¼Œæ‰€ä»¥æ•´åˆæ—¶éœ€è¦è°ƒæ•´ï¼Œé¿å…é‡åˆ

    :param data375:
    :param data110:
    :return:
    """
    data110 = data110.reset_index()
    data110['PATIENT_ID'] += 375
    data110 = data110.set_index(['PATIENT_ID', 'RE_DATE'])
    data = data375.append(data110)
    return data
```


## å¸¸ç”¨å‚æ•°
```{python}
# å¸¸ç”¨å‚æ•°
top3_feats_cols = ['ä¹³é…¸è„±æ°¢é…¶', 'è¶…æ•Cååº”è›‹ç™½', 'æ·‹å·´ç»†èƒ(%)']
in_out_time_cols = ['å…¥é™¢æ—¶é—´', 'å‡ºé™¢æ—¶é—´']
path_train = 'analysis/data/raw_data/time_series_375_prerpocess.xlsx'
path_test = 'analysis/data/raw_data/time_series_test_110_preprocess.xlsx'
```


## analysis

### ğŸ“Œ  features_selection

åˆ†ä¸¤æ­¥:

1. å…ˆç”¨XGBoostç®—ç‰¹å¾çš„é‡è¦æ€§, æ³¨æ„è¿™ä¸ªè¿‡ç¨‹é‡å¤ 100 æ¬¡, å–çš„å¹³å‡å€¼, è¿™æ ·å°±å¾—åˆ°äº†ç‰¹å¾çš„é‡è¦æ’åº

[æ¯æ¬¡æ‹Ÿåˆä¸€æ¬¡æ¨¡å‹éƒ½æœ‰ä¸€ä¸ªç‰¹å¾é‡è¦æ€§åˆ—è¡¨, æˆ‘ä»¬æŠŠè¿™ä¸ªè¿‡ç¨‹é‡å¤ 100 æ¬¡(ä¸»è¦æ˜¯å› ä¸ºæ ·æœ¬é‡å¤ªå°‘),ç®—ä¸€ä¸ªå¹³å‡å€¼, æ³¨æ„æ¯æ¬¡çš„ éšæœºç§å­éƒ½æ˜¯ä¸ä¸€æ ·çš„, è¿™æ ·èƒ½ä¿è¯æ•°æ®é›†æ˜¯æ‹†åˆ†æ˜¯ä¸åŒçš„]

2. æœ‰ç‚¹ç±»ä¼¼äº IFS çš„å·¥ä½œ, ä»æŒ‰ç…§é‡è¦æ’åºå¥½çš„ç‰¹å¾, é€æ­¥å¢åŠ ç‰¹å¾, ç›´åˆ°æ€§èƒ½ä¸èƒ½æå‡ä¸ºæ­¢, è¿™æ ·å°±èƒ½æ‰¾åˆ°éœ€è¦å‡ ä¸ªç‰¹å¾æ¥åšæœ€åçš„æ¨¡å‹(æ³¨æ„è¿™é‡Œäº‹å…ˆå°±åªé€‰å‰10 ä¸ªç‰¹å¾æ¥è¿›è¡Œé€æ­¥ç‰¹å¾é€‰æ‹©), ç¡®å®šæ˜¯è¿™ä¹Ÿæ˜¯æŒ‰ç…§é¡ºåºæ¥é€‰çš„, æ‰€ä»¥ä¹‹å‰çš„é‡è¦æ€§æ’åºå¾ˆé‡è¦, è¿™ä¸ªè¦æ˜¯ä¸å‡†çš„è¯, ä¼šé€‰ä¸€äº›æ²¡ç”¨çš„ç‰¹å¾è¿›å»

è¾“å…¥: `data_read_and_split`: ä¼šè®¡ç®—å‡º, X, y, feature name
è¾“å‡º: æœ€ä¸­å…¥é€‰çš„ç‰¹å¾åå­—(list)



```{python}
## features selection part
def features_selection():
    ## è¯»å–375çš„æ•°æ®
    X_data_all_features,Y_data,x_col = data_read_and_split()
    # name_dict = {'ä¹³é…¸è„±æ°¢é…¶':'Lactate dehydrogenase (LDH)','æ·‹å·´ç»†èƒ(%)':'Lymphocytes(%)','è¶…æ•Cååº”è›‹ç™½':'High-sensitivity C-reactive protein (hs-CRP)',
    #          'é’ ':'Sodium','æ°¯':'Chlorine','å›½é™…æ ‡å‡†åŒ–æ¯”å€¼':'International Normalized Ratio (INR)','å—œé…¸ç»†èƒ(#)':'Eosinophils(#)',
    #          'å—œé…¸ç»†èƒ(%)':'Eosinophils(%)','å•æ ¸ç»†èƒ(%)':'Monocytes(%)','ç™½è›‹ç™½':'Albumin'}
    #
    # æ„å»ºä¸€ä¸ªdataframeç”¨äºå­˜å‚¨ç‰¹å¾çš„é‡è¦ç¨‹åº¦ä¿¡æ¯
    import_feature = pd.DataFrame()
    import_feature['col'] = x_col
    import_feature['xgb'] = 0
    # é‡å¤100æ¬¡è¯•éªŒ
    for i in range(100): # 50,150
        #æ¯æ¬¡è¯•éªŒå°†375æ•°æ®éšæœºåˆ’åˆ†0.7è®­ç»ƒé›†å’Œ0.3æµ‹è¯•é›†ï¼Œæ³¨æ„éšæœºrandom_state=i
        ## æ³¨æ˜ï¼šæ­¤æ–¹æ³•åŸå› æ˜¯ç”±äºå¯è·å¾—çš„æ ·æœ¬é‡è¾ƒå°‘ï¼Œä¸ºäº†äº§ç”Ÿä¸åŒçš„è®­ç»ƒæ ·æœ¬é›†ï¼Œä½¿å¾—ç‰¹å¾çš„é‡è¦åº¦æ’åºæ›´ä¸ºç¨³å®šï¼Œä»è€Œé€‰æ‹©äº†è¿™æ ·ä¸€ç§æ–¹å¼ã€‚
        ## é€šè¿‡æ¯æ¬¡ä¸åŒçš„éšæœºç§å­äº§ç”Ÿä¸åŒçš„æ ·æœ¬ï¼Œä»è€Œè¾¾åˆ°ä¸€å®šç¨‹åº¦ä¸Šçš„æŠ‘åˆ¶å°‘é‡æ ·æœ¬çš„å¼‚å¸¸å¯¹ç‰¹å¾çš„é‡è¦åº¦å¸¦æ¥çš„å½±å“ã€‚
        x_train, x_test, y_train, y_test = train_test_split(X_data_all_features, Y_data, test_size=0.3, random_state=i)
        #å®šä¹‰æ¨¡å‹è¶…å‚æ•°
        model = xgb.XGBClassifier(
                max_depth=4
                ,learning_rate=0.2
                ,reg_lambda=1
                ,n_estimators=150
                ,subsample = 0.9
                ,colsample_bytree = 0.9)
        #æ¨¡å‹æ‹Ÿåˆ
        model.fit(x_train, y_train)
        #ç´¯åŠ ç‰¹å¾é‡è¦ç¨‹åº¦
        import_feature['xgb'] = import_feature['xgb']+model.feature_importances_/100
    # æŒ‰ç…§ç‰¹å¾é‡è¦ç¨‹åº¦ï¼Œé™åºæ’åˆ—
    import_feature = import_feature.sort_values(axis=0, ascending=False, by='xgb')
    print('Top 10 features:')
    print(import_feature.head(10))
    # Sort feature importances from GBC model trained earlier
    # æŒ‰ç…§ç‰¹å¾é‡è¦ç¨‹åº¦çš„ä½ç½®ä¿¡æ¯
    indices = np.argsort(import_feature['xgb'].values)[::-1]
    #è·å–å‰10ä¸ªé‡è¦çš„ç‰¹å¾ä½ç½®
    Num_f = 10
    indices = indices[:Num_f]
    
    # Visualise these with a barplot
    # plt.subplots(dpi=400,figsize=(12, 10))
    plt.subplots(figsize=(12, 10))
    # g = sns.barplot(y=list(name_dict.values())[:Num_f], x = import_feature.iloc[:Num_f]['xgb'].values[indices], orient='h') #import_feature.iloc[:Num_f]['col'].values[indices]
    g = sns.barplot(y=import_feature.iloc[:Num_f]['col'].values[indices], 
                    x = import_feature.iloc[:Num_f]['xgb'].values[indices], 
                    orient='h') #import_feature.iloc[:Num_f]['col'].values[indices]
    g.set_xlabel("Relative importance",fontsize=18)
    g.set_ylabel("Features",fontsize=18)
    g.tick_params(labelsize=14)
    sns.despine() 
    # plt.savefig('feature_importances_v3.png')
    plt.show()
    # g.set_title("The mean feature importance of XGB models");
    # è·å–å‰10é‡è¦ç‰¹å¾çš„é‡è¦æ€§æ•°å€¼
    import_feature_cols= import_feature['col'].values[:10]

    # ç”»ç‰¹å¾é‡‘å­—å¡”
    num_i = 1
    val_score_old = 0
    val_score_new = 0
    while val_score_new >= val_score_old:
        val_score_old = val_score_new
        # æŒ‰é‡è¦ç¨‹åº¦é¡ºåºå–ç‰¹ç§
        x_col = import_feature_cols[:num_i]
        print(x_col)
        X_data = X_data_all_features[x_col]#.values
        ## äº¤å‰éªŒè¯
        print('5-Fold CV:')
        acc_train, acc_val, acc_train_std, acc_val_std = StratifiedKFold_func_with_features_sel(X_data.values,Y_data.values)
        print("Train AUC-score is %.4f ; Validation AUC-score is %.4f" % (acc_train,acc_val))
        print("Train AUC-score-std is %.4f ; Validation AUC-score-std is %.4f" % (acc_train_std,acc_val_std))
        val_score_new = acc_val
        num_i += 1
        
    print('Selected features:',x_col[:-1])
    
    return list(x_col[:-1])
```

### run feature selection
```{python}
res = features_selection()
```

```{r}
py$res
```

```{r}
View(py$res)
```

### R version

### load data: X , y
```{python}
## features selection part
# features_selection():
## è¯»å–375çš„æ•°æ®
X_data_all_features,Y_data,x_col = data_read_and_split()
```


### MRMR: feature rank dataframe 

ç‰¹å¾æ’åºçŸ©é˜µ, è·Ÿ MRMRçš„åŠŸèƒ½å·®ä¸å¤š, åœ¨æ•°æ®é‡æ¯”è¾ƒå°çš„æ—¶å€™

```{python}
def features_rank(X, y):
  # æ„å»ºä¸€ä¸ªdataframeç”¨äºå­˜å‚¨ç‰¹å¾çš„é‡è¦ç¨‹åº¦ä¿¡æ¯
  import_feature = pd.DataFrame()
  import_feature['col'] = X.columns.tolist()
  import_feature['model'] = 0
  # é‡å¤100æ¬¡è¯•éªŒ
  for i in range(100): # 50,150
      #æ¯æ¬¡è¯•éªŒå°†375æ•°æ®éšæœºåˆ’åˆ†0.7è®­ç»ƒé›†å’Œ0.3æµ‹è¯•é›†ï¼Œæ³¨æ„éšæœºrandom_state=i
      ## æ³¨æ˜ï¼šæ­¤æ–¹æ³•åŸå› æ˜¯ç”±äºå¯è·å¾—çš„æ ·æœ¬é‡è¾ƒå°‘ï¼Œä¸ºäº†äº§ç”Ÿä¸åŒçš„è®­ç»ƒæ ·æœ¬é›†ï¼Œä½¿å¾—ç‰¹å¾çš„é‡è¦åº¦æ’åºæ›´ä¸ºç¨³å®šï¼Œä»è€Œé€‰æ‹©äº†è¿™æ ·ä¸€ç§æ–¹å¼ã€‚
      ## é€šè¿‡æ¯æ¬¡ä¸åŒçš„éšæœºç§å­äº§ç”Ÿä¸åŒçš„æ ·æœ¬ï¼Œä»è€Œè¾¾åˆ°ä¸€å®šç¨‹åº¦ä¸Šçš„æŠ‘åˆ¶å°‘é‡æ ·æœ¬çš„å¼‚å¸¸å¯¹ç‰¹å¾çš„é‡è¦åº¦å¸¦æ¥çš„å½±å“ã€‚
      x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)
      #å®šä¹‰æ¨¡å‹è¶…å‚æ•°
      model = xgb.XGBClassifier(
              max_depth=4
              ,learning_rate=0.2
              ,reg_lambda=1
              ,n_estimators=150
              ,subsample = 0.9
              ,colsample_bytree = 0.9)
      #æ¨¡å‹æ‹Ÿåˆ
      model.fit(x_train, y_train)
      #ç´¯åŠ ç‰¹å¾é‡è¦ç¨‹åº¦
      import_feature['model'] = import_feature['model'] + model.feature_importances_/100
  # æŒ‰ç…§ç‰¹å¾é‡è¦ç¨‹åº¦ï¼Œé™åºæ’åˆ—
  return import_feature

feature_rank_res = features_rank(X_data_all_features, Y_data)
```

```{r}
py$feature_rank_res %>% view()
```

```{r}
# è·å–å‰10é‡è¦ç‰¹å¾çš„é‡è¦æ€§æ•°å€¼
import_feature_cols <- 
  py$import_feature %>% 
  arrange(desc(xgb)) %>% 
  head(10) %>% 
  pull(col)
```

### IFS

```{python}
# ç”»ç‰¹å¾é‡‘å­—å¡”
import_feature_cols = r.import_feature_cols

num_i = 1
val_score_old = 0
val_score_new = 0
while val_score_new >= val_score_old:
    val_score_old = val_score_new
    # æŒ‰é‡è¦ç¨‹åº¦é¡ºåºå–ç‰¹ç§
    x_col = import_feature_cols[:num_i]
    print(x_col)
    X_data = X_data_all_features[x_col]#.values
    ## äº¤å‰éªŒè¯
    print('5-Fold CV:')
    acc_train, acc_val, acc_train_std, acc_val_std = StratifiedKFold_func_with_features_sel(X_data.values, Y_data.values)
    print("Train AUC-score is %.4f ; Validation AUC-score is %.4f" % (acc_train,acc_val))
    print("Train AUC-score-std is %.4f ; Validation AUC-score-std is %.4f" % (acc_train_std,acc_val_std))
    val_score_new = acc_val
    num_i += 1
    
print('Selected features:',x_col[:-1])
x_col[:-1]
```

```{python}
def IFS_K_fold(import_feature_cols, X, y):
  acc_train = [None] * 10
  acc_val = [None] * 10
  acc_train_std = [None] * 10
  acc_val_std = [None] * 10

  for num_i in range(0, len(import_feature_cols)):
    print(num_i)
    # æŒ‰é‡è¦ç¨‹åº¦é¡ºåºå–ç‰¹ç§
    x_col = import_feature_cols[:num_i + 1]
    print(x_col)
    X_select = X[x_col]#.values
    ## äº¤å‰éªŒè¯
    print('5-Fold CV:')
    acc_train[num_i], acc_val[num_i], acc_train_std[num_i], acc_val_std[num_i] = StratifiedKFold_func_with_features_sel(X_select.values, y.values)
    
  res = pd.DataFrame(
      {'acc_train': acc_train,
       'acc_val': acc_val,
       'acc_train_std': acc_train_std,
       'acc_val_std': acc_val_std
      })
  return res

```

```{python}
IFS_res = IFS_K_fold(import_feature_cols, X_data_all_features, Y_data)

```


### ğŸ“Œ  single_tree

```{python}
def single_tree(cols=['ä¹³é…¸è„±æ°¢é…¶','æ·‹å·´ç»†èƒ(%)','è¶…æ•Cååº”è›‹ç™½']):
    print('single_tree:\n')
    #è·å–375ç—…äººï¼ˆdata_df_unnaï¼‰ å’Œ110ç—…äººï¼ˆdata_pre_dfï¼‰æ•°æ®
    data_df_unna,data_pre_df = data_preprocess(path_train, path_test)
    #å»æ‰å…¨ç©ºè¡Œï¼Œæ­¤æ—¶375æ€»æ•°ç›®å˜æˆ351
    data_df_unna = data_df_unna.dropna(subset=cols,how='any')

    cols.append('Type2')
    #è·å–ç—…äººçš„ç»“å±€æ ‡ç­¾
    Tets_Y = data_pre_df.reset_index()[['PATIENT_ID','å‡ºé™¢æ–¹å¼']].copy()
    #ä¿®æ”¹dataframeçš„åå­—
    Tets_Y = Tets_Y.rename(columns={'PATIENT_ID': 'ID', 'å‡ºé™¢æ–¹å¼': 'Y'})
    # è·å–110ç—…äººçš„æ ‡ç­¾æ•°æ®
    y_true = Tets_Y['Y'].values

    x_col = cols[:-1]
    y_col = cols[-1]
    # è·å–351ç—…äººçš„ä¸‰ç‰¹å¾æ•°æ®
    x_np = data_df_unna[x_col].values
    # è·å–351ç—…äººçš„æ ‡ç­¾æ•°æ®
    y_np = data_df_unna[y_col].values
    # è·å–110ç—…äººçš„ä¸‰ç‰¹å¾æ•°æ®
    x_test = data_pre_df[x_col].values
    # åœ¨351ç—…äººä¸Šåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œæ­¤æ—¶110è§†ä¸ºæµ‹è¯•é›†
    X_train, X_val, y_train, y_val = train_test_split(x_np, y_np, test_size=0.3, random_state=6)
    #é™å®šå•æ ‘xgbæ¨¡å‹
    model = xgb.XGBClassifier(
        max_depth=3,
        n_estimators=1,
    )
    model.fit(X_train,y_train)

    #è®­ç»ƒé›†æ··æ·†çŸ©é˜µ
    pred_train = model.predict(X_train)
    show_confusion_matrix(y_train, pred_train)
    print(classification_report(y_train, pred_train))

    #éªŒè¯é›†æ··æ·†çŸ©é˜µ
    pred_val = model.predict(X_val)
    show_confusion_matrix(y_val, pred_val)
    print(classification_report(y_val, pred_val))
    
    #æµ‹è¯•é›†æ··æ·†çŸ©é˜µ
    pred_test = model.predict(x_test)
    print('True test label:',y_true)
    print('Predict test label:',pred_test.astype('int32'))
    show_confusion_matrix(y_true, pred_test)
    print(classification_report(y_true, pred_test))
    
    plt.figure(dpi=300,figsize=(8,6))
    plot_tree(model)
    plt.show()
    
    graph = xgb.to_graphviz(model)
    graph.render(filename='single-tree.dot')
    #å•æ ‘å¯è§†åŒ–
    #ceate_feature_map(cols[:-1])
    #graph = xgb.to_graphviz(model, fmap='xgb.fmap', num_trees=0, **{'size': str(10)})
    #graph.render(filename='single-tree.dot')

```

```{python}
single_tree()
```


### load data

```{r}
train <- rio::import("analysis/data/raw_data/jixing/train_357.csv")
validation <- rio::import("analysis/data/raw_data/jixing/validation_110.csv")

cols <- c('ä¹³é…¸è„±æ°¢é…¶','æ·‹å·´ç»†èƒ(%)','è¶…æ•Cååº”è›‹ç™½')

train_x <- select(train, all_of(cols)) %>% as.matrix()
train_y  <- select(train, "label") %>% as.matrix()

validation_x <- select(validation, all_of(cols)) %>% as.matrix()
validation_y  <- select(validation, "å‡ºé™¢æ–¹å¼") %>% as.matrix()
```

### single tree with R

```{python}

# åœ¨351ç—…äººä¸Šåˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œæ­¤æ—¶110è§†ä¸ºæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(r.train_x, r.train_y, test_size=0.3, random_state=6)
#é™å®šå•æ ‘xgbæ¨¡å‹
model = xgb.XGBClassifier(
    max_depth=3,
    n_estimators=1,
)
model.fit(X_train,y_train)

#è®­ç»ƒé›†æ··æ·†çŸ©é˜µ
pred_train = model.predict(X_train)
show_confusion_matrix(y_train, pred_train)
print(classification_report(y_train, pred_train))

#æµ‹è¯•é›†æ··æ·†çŸ©é˜µ
pred_test = model.predict(X_test)
show_confusion_matrix(y_test, pred_test)
print(classification_report(y_test, pred_test))
    
#å¤–éƒ¨é›†æ··æ·†çŸ©é˜µ
pred_test = model.predict(r.validation_x)
print('True test label:',r.validation_y)
print('Predict test label:',pred_test.astype('int32'))
show_confusion_matrix(r.validation_y, pred_test)
print(classification_report(r.validation_y, pred_test))
    
plt.figure(dpi=300,figsize=(8,6))
plot_tree(model)
plt.show()
    
graph = xgb.to_graphviz(model)
graph.render(filename='~/Desktop/single-tree.dot')
#å•æ ‘å¯è§†åŒ–
def ceate_feature_map(features):
    outfile = open('xgb.fmap', 'w')
    i = 0
    for feat in features:
        outfile.write('{0}\t{1}\tq\n'.format(i, feat))
        i = i + 1
    outfile.close()

ceate_feature_map(r.cols)
graph = xgb.to_graphviz(model, fmap='xgb.fmap', num_trees=0, **{'size': str(10)})
graph.render(filename='single-tree.dot')

```




### ğŸ“Œ  Compare_with_other_method

é›†ä¸­æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ¯”è¾ƒ

```{python}
def Compare_with_other_method(sub_cols=['ä¹³é…¸è„±æ°¢é…¶','æ·‹å·´ç»†èƒ(%)','è¶…æ•Cååº”è›‹ç™½']):
    # è¯»å–351æ•°æ®é›†ï¼ˆä»375ä¸­åˆ é™¤sub_colså…¨ä¸ºç©ºçš„æ ·æœ¬å¾—åˆ°ï¼‰
    x_np,y_np,x_col = data_read_and_split(is_dropna=True,sub_cols=sub_cols)

    #ä¸ºäº†siçš„å›¾4è¯´æ˜é—®é¢˜ã€‚å¦‚æœæ˜¯5æŠ˜ï¼Œç”»ä¸å‡ºSI å›¾4
    X_train, X_val, y_train, y_val = train_test_split(x_np, y_np, test_size=0.3, random_state=6)

    #å®šä¹‰å…¨ç‰¹å¾ä¸‹çš„æ¯”å¯¹æ–¹æ³•
    xgb_n_clf = xgb.XGBClassifier(
        max_depth=4
        ,learning_rate=0.2
        ,reg_lambda=1
        ,n_estimators=150
        ,subsample = 0.9
        ,colsample_bytree = 0.9
        ,random_state=0)
    tree_clf = tree.DecisionTreeClassifier(random_state=0,max_depth=4) #random_state=0,ä¹‹å‰æ²¡åŠ 
    RF_clf1 = RandomForestClassifier(random_state=0,n_estimators=150,max_depth=4,)
    LR_clf = linear_model.LogisticRegression(random_state=0,C=1,solver='lbfgs')
    LR_reg_clf = linear_model.LogisticRegression(random_state=0,C=0.1, solver='lbfgs')
    
    fig = plt.figure(dpi=400,figsize=(16, 8))

    Num_iter = 100
    
    i = 0
    labels_names = []
    Moodel_name = ['Multi-tree XGBoost with all features',
                   'Decision tree with all features',
                   'Random Forest with all features',
                   'Logistic regression with all features with regularization parameter = 1 (by default)',
                   'Logistic regression with all features with regularization parameter = 10',]
    for model in [xgb_n_clf,tree_clf,RF_clf1,LR_clf,LR_reg_clf]:
        print('Model:'+Moodel_name[i])
        #ä»¥f1çš„è¯„ä»·æ–¹å¼æ¥kæŠ˜
        acc_train, acc_val, acc_train_std, acc_val_std = StratifiedKFold_func(x_np.values, y_np.values,Num_iter,model, score_type ='f1')
        #print('F1-score of Train:%.6f with std:%.4f \nF1-score of Validation:%.4f with std:%.6f '%(acc_train,acc_train_std,acc_val,acc_val_std))
        # ä»¥aucçš„è¯„ä»·æ–¹å¼æ¥kæŠ˜
        acc_train, acc_val, acc_train_std, acc_val_std = StratifiedKFold_func(x_np.values, y_np.values,Num_iter,model, score_type ='auc')
        print('AUC of Train:%.6f with std:%.4f \nAUC of Validation:%.6f with std:%.4f '%(acc_train,acc_train_std,acc_val,acc_val_std))

        #ä¸ºäº†ç”»siå›¾4
        model.fit(X_train,y_train)
        pred_train_probe = model.predict_proba(X_train)[:,1]
        pred_val_probe = model.predict_proba(X_val)[:,1]
        #plot_roc(y_val, pred_val_probe,Moodel_name[i],fig,labels_names,i) # ä¸ºäº†ç”»siå›¾4ä¸­çš„test
        plot_roc(y_train, pred_train_probe,Moodel_name[i],fig,labels_names,i) # ä¸ºäº†ç”»siå›¾4 train
        print('AUC socre:',roc_auc_score(y_val, pred_val_probe))
        
        i = i+1

    ## ä¸‰ç‰¹å¾çš„å•æ ‘æ¨¡å‹å¯¹æ¯”
    x_np_sel = x_np[sub_cols] #é€‰æ‹©ä¸‰ç‰¹å¾
    ## åˆ’åˆ†æ•°æ®é›†æ˜¯ä¸ºäº†å•æ ‘çš„å•æ¬¡è®­ç»ƒå¹¶ç”ŸæˆAUCå›¾ï¼Œåˆ’åˆ†æ–¹å¼å’Œä¹‹å‰ä¿å­˜ä¸€è‡´ã€‚
    X_train, X_val, y_train, y_val = train_test_split(x_np_sel, y_np, test_size=0.3, random_state=6) 

    #ä¸ºäº†ä¸‰ç‰¹å¾çš„æ¨¡å‹å¯¹æ¯”
    xgb_clf = xgb.XGBClassifier(
        max_depth=3,
        n_estimators=1,
        random_state=0,
    )
    
    tree_clf = tree.DecisionTreeClassifier(random_state=0,max_depth=3)
    RF_clf2 = RandomForestClassifier(random_state=0,n_estimators=1,max_depth=3,)

    #i = 0
    Moodel_name = ['Single-tree XGBoost with three features',
                   'Decision tree with three features',
                   'Random Forest with a single tree constraint with three features',]
    for model in [xgb_clf,tree_clf,RF_clf2]:
        print('Model'+Moodel_name[i-5])
        #f1çš„ç»“æœ
        acc_train, acc_val, acc_train_std, acc_val_std = StratifiedKFold_func(x_np_sel.values, y_np.values,Num_iter,model, score_type ='f1')
        #print('F1-score of Train:%.6f with std:%.4f \nF1-score of Validation:%.4f with std:%.6f '%(acc_train,acc_train_std,acc_val,acc_val_std))
        #aucçš„ç»“æœ
        acc_train, acc_val, acc_train_std, acc_val_std = StratifiedKFold_func(x_np_sel.values, y_np.values,Num_iter,model, score_type ='auc')
        print('AUC of Train:%.6f with std:%.4f \nAUC of Validation:%.6f with std:%.4f '%(acc_train,acc_train_std,acc_val,acc_val_std))
        
        model.fit(X_train,y_train)
        pred_train_probe = model.predict_proba(X_train)[:,1]    # ä¸ºäº†ç”»siå›¾4ä¸­çš„train
        pred_val_probe = model.predict_proba(X_val)[:,1]    # ä¸ºäº†ç”»siå›¾4ä¸­çš„test
        #plot_roc(y_val, pred_val_probe,Moodel_name[i-5],fig,labels_names,i) # ä¸ºäº†ç”»siå›¾4ä¸­çš„test
        plot_roc(y_train, pred_train_probe,Moodel_name[i-5],fig,labels_names,i)# ä¸ºäº†ç”»siå›¾4ä¸­çš„train
        print('AUC socre:',roc_auc_score(y_val, pred_val_probe))
        
        i = i+1
    
    plt.plot([0,1],[0,1],'r--')
    plt.legend(loc='SouthEastOutside', fontsize=14)
    plt.savefig('AUC_train.png')
    plt.show()


```

```{python}
res
```

```{python}
Compare_with_other_method(sub_cols=['ä¹³é…¸è„±æ°¢é…¶', 'æ·‹å·´ç»†èƒ(%)', 'è¶…æ•Cååº”è›‹ç™½', 'é’ '])
```



##  ğŸ³  

## data clean
## data load
## MRMR
## IFS
## single tree
## compare with other model




## DATA explore

è¿™ä¹ˆå¤šçš„ç¼ºå¤±æ€ä¹ˆå¤„ç†? åˆ é™¤å“ªäº›æ ·æœ¬  

è¿™ä¹ˆå¤šçš„ä¸è§„åˆ™åˆ†å¸ƒæ€ä¹ˆå¤„ç†

## to do list

+ scikit-learn `classification_report`å‡½æ•°çš„è¾“å‡º, ç»“æœè½¬æ¢, è¿™ä¸ªä¹‹å‰æˆ‘å†™è¿‡, python çš„ç»“æœè½¬æ¢æˆ R é‡Œé¢çš„ dataframe ç»“æœ, è¿™æ ·çš„è¯åˆ©äºåç»­çš„å›¾è¡¨åˆ¶ä½œ, å…³é”®æ˜¯è¦å»ºä¸€ä¸ªR åŒ…, æŠŠè¿™äº›å‡½æ•°å…¨éƒ¨æ”¾åˆ°é‡Œé¢, è¿™æ ·ä»¥åå°±å¯ä»¥ç›´æ¥è°ƒç”¨: æˆ‘ä¹‹å‰å¥½åƒå†™äº†ä¸€ä¸ªè¿™æ ·çš„ RåŒ…, é‡Œé¢éƒ½æ˜¯ä¸€äº› python å‡½æ•°(åº”è¯¥éƒ½æ˜¯ä¸€äº› RDKit çš„å‡½æ•°), ä½ è¦åšçš„å°±æ˜¯ç›´æ¥è°ƒç”¨. å¦å¤–è¦ä¿è¯å¯ä»¥ç›´æ¥è°ƒç”¨è€Œä¸éœ€è¦åœ¨åŠ è½½ä¸€é, å¦å¤–é‡Œé¢çš„ä¸€äº›åŠŸèƒ½æ˜¯ç”¨æ¥æ–¹ä¾¿åˆ¶ä½œå›¾è¡¨çš„, æˆ–è€…ç›´æ¥åµŒå…¥åˆ° manuscriptJXè¿™ä¸ªåŒ…é‡Œ?



+ æƒ³åŠæ³•æŠŠè¿™ä¸ªåˆ†æå˜æˆæ ‡å‡†æµç¨‹, è€Œä¸æ˜¯åªèƒ½ç”¨åœ¨è¿™ä¸€ä¸ªé¡¹ç›®ä¸Š, å€Ÿé‰´ä»–çš„åˆ†ææ€è·¯æ˜¯å¯ä»¥, ä½†æ˜¯ä»–è¿™ä¸ªä¸æ¡†æ¶åŒ–çš„åšæ³•,æ˜¯åœ¨æ˜¯ä¸å¯å–.












